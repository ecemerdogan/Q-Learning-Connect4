{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AF9cPyhrx7V-"
   },
   "source": [
    "\n",
    "# **Reinforcement Learning**\n",
    "\n",
    "**Introduction to RL**\n",
    "\n",
    "Reinforcement learning has again become one of the most popular studies in the last 10 years. Reinforcement Learning is a subtopic of machine learning that combines it with control theory. This learning concept contains an environment and its agent which interacts with the environment. Based on the states that are taken from the environment, the agent acts. As a result of these actions, the environment gives the agent reward which can be positive or negative. In this field, properties of the environment that have modelled or designed is important to what type of learning will be used. For instance, reinforcement learning might be evaluated under two main subtitles named as model based and model free reinforcement learning. The choice between them should be selected based on the requirements of the environment.\n",
    "\n",
    "Note: I will add the main elements of RL (such as what is a state, action or transition function etc.) to this introduction part.\n",
    "\n",
    "**Q-Learning Algorithm**\n",
    "    \n",
    "Q-Learning algorithm is a model-free technique that trains the machine for each step in an episode of a game. In this learning technique, the Q-function tells you in a current state which reward is received when an agent acts is used. The formula of Q-function is given down below:\n",
    "\n",
    ">$$\n",
    "Q(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) \\max_{a'} Q(s', a')\n",
    "$$\n",
    "\n",
    "Before explaining the equation of Q function, in order to clarify the working principle of the Q -learning algorithm, it is essential to learn what is a policy. As it is understood by the mean of policy, this concept is used to represent the behaviour of an agent in a given state and an environment.\n",
    "\n",
    "In the Q-function, the reward obtained for the next state is computed by adding the reward obtained in the current state to the sum of the probabilities of selecting each action multiplied by the maximum expected future reward. There is a ùõæ variable named as discount factor that balances the significance of immediate and future rewards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
