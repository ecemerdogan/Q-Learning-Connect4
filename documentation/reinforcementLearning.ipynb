
# **Reinforcement Learning**

**Introduction to RL**

Reinforcement learning has again become one of the most popular studies in the last 10 years. Reinforcement Learning is a subtopic of machine learning that combines it with control theory. This learning concept contains an environment and its agent which interacts with the environment. The agent acts based on the states taken from the environment. As a result of these actions, the environment gives the agent a reward which can be positive or negative. 

In this field, the properties of the environment that have been modeled or designed are important to what type of learning will be used. From this point of view, reinforcement learning is, also, be evaluated under two main subtitles named as model-based and model-free reinforcement learning.  Besides Model-based RL uses Dynamic programming techniques,  a Q-learning algorithm that uses the temporal difference learning approach can be given as an example in Model-free RL. The choice between them should be selected based on the dynamics of the environment. In model-based RL, the probabilities of the next state when obtainable action is taken in a current state and the reward of each action for the agent will be known, completely. However, an agent gains the experience of learning the optimal policy of the agent through trial and error in an unknown environment, which is named model-free RL. In this concept is much more reliable and proper for real-world problems and their applications because of the complexity of the environment. It is also crucial to understand what is temporal difference approach and what is the point of this learning approach is. It is a concept that ...

Note: I will add the main elements of RL (such as what is a state, action or transition function, etc.) to this introduction part.

**Q-Learning Algorithm**
    
Q-learning algorithm is a model-free technique that trains the machine for each step in an episode of a game. In this learning technique, the Q-function tells you in a current state which reward is received when an agent acts is used.

The formula of the Q-function is given below:

>$$
Q(s, a) = R(s, a) + \gamma \sum_{s'} P(s'|s, a) \max_{a'} Q(s', a')
$$

Before explaining the equation of the Q-function, to clarify the working principle of the Q -learning algorithm, it is essential to learn what is a policy. As it is understood by the mean of policy, this concept is used to represent the behaviour of an agent in a given state and an environment. 

In the Q-function, the reward obtained for the next state is computed by adding the reward obtained in the current state to the sum of the probabilities of selecting each action multiplied by the maximum expected future reward. There is a ùõæ variable named as discount factor that balances the significance of immediate and future rewards. 

