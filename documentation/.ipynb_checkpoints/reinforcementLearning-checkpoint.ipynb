{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AF9cPyhrx7V-"
   },
   "source": [
    "\n",
    "# **Reinforcement Learning**\n",
    "\n",
    "**Introduction to RL**\n",
    "\n",
    "Reinforcement learning has again become one of the most popular studies in the last 10 years. Reinforcement Learning is a subtopic of machine learning that combines it with control theory. This learning concept contains an environment and its agent which interacts with the environment. The agent acts based on the states taken from the environment. As a result of these actions, the environment gives the agent a reward which can be positive or negative. \n",
    "\n",
    "In this field, properties of the environment that have been modeled or designed are important to what type of learning will be used. From this point of view, reinforcement learning is, also, be evaluated under two main subtitles: model-based and model-free reinforcement learning.  Besides Model-based RL uses Dynamic programming techniques,  a Q-learning algorithm that uses a temporal difference learning approach can be given as an example in Model-free RL. The choice between them should be selected based on the dynamics of the environment. In model-based RL, the probabilities of the next state when obtainable action is taken in a current state and the reward of each action for the agent will be known, completely. However, an agent gains the experience of learning the optimal policy of the agent through trial and error in an unknown environment, which is named model-free RL. This concept is much more reliable and proper for real-world problems and their applications because of the complexity of the environment. It is also crucial to understand what is temporal difference approach is and what the point of this learning approach is. It is a concept that ...\n",
    "\n",
    "Note: I will add the main elements of RL (such as what is a state, action or transition function, etc.) to this introduction part.\n",
    "\n",
    "**Q-Learning Algorithm**\n",
    "    \n",
    "Q-learning algorithm is a model-free technique that trains the machine for each step in an episode of a game. In this learning technique, the Q-function tells you in a current state which reward is received when an agent acts is used.\n",
    "\n",
    "The formula of the Q-function is given below:\n",
    "\n",
    ">$$\n",
    "Q(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) \\max_{a'} Q(s', a')\n",
    "$$\n",
    "\n",
    "Before explaining the equation of the Q function, to clarify the working principle of the Q -learning algorithm, it is essential to learn what is a policy. As it is understood by the means of policy, this concept is used to represent the behavior of an agent in a given state and environment. \n",
    "\n",
    "In the Q-function, the reward obtained for the next state is computed by adding the reward obtained in the current state to the sum of the probabilities of selecting each action multiplied by the maximum expected future reward. There is a ð›¾ variable named a discount factor that balances the significance of immediate and future rewards. \n",
    "\n",
    "**Monte Carlo Tree Search**\n",
    "\n",
    "Monte Carlo Tree Search (MCTS) is a heuristic search algorithm that uses game trees. These represent all of the states of a combinatorial game such as chess, Go, or tic tac toe. This algorithm uses UCB policy as shown in the equation below:\n",
    "\n",
    ">$$\n",
    "UCB(s_i) = v_i + C \\sqrt(log(N)/n_i)\n",
    "$$\n",
    ">$v_{i}$: Mean value of the node which represents the exploitation term.\n",
    ">\n",
    ">$C$: Constant.\n",
    ">\n",
    ">$N$: Total number of simulations done for the parent node.\n",
    ">\n",
    ">$n_{i}$: Number of simulations done for the current child node.\n",
    ">\n",
    ">These $N$ and $n_{i}$ terms form the exploration term which is given in the MCTS algorithm equation as $\\sqrt(\\log(N)/n_i)$\n",
    "\n",
    "For each layer, the MCTS algorithm uses the highest UCB value to choose the next node instead of full exploration in the environment as brute force approaches do. \n",
    "\n",
    "MCTS has four distinct and iterative steps:\n",
    "\n",
    ">**1.Selection (Tree Traversal):**\n",
    ">\n",
    ">**2.Node Expansion:**\n",
    ">\n",
    ">**3.Random Simulation (Rollout):**\n",
    ">\n",
    ">**4.Backpropagation:**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
